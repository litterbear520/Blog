---
sidebar_position: 8
description: 使用 LangChain 调用聊天模型
---

# 调用聊天模型

聊天消息包含下面几种类型，使用时需要按照约定传入合适的值：

- **AIMessage**：就是 AI 输出的消息，可以是针对问题的回答。（OpenAI库中的`assistant`角色）
- **HumanMessage**：人类消息就是用户信息，由人给出的信息发送给LLMs的提示信息，比如“实现一个快速排序方法”。（OpenAI库中的`user`角色）
- **SystemMessage**：可以用于指定模型具体所处的环境和背景，如角色扮演等。你可以在这里给出具体的指示，比如“作为一个代码专家”，或者“返回json格式”。（OpenAI库中的`system`角色）

## 演示单独使用HumanMessage

```python
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.messages import HumanMessage
from dotenv import load_dotenv
import os

load_dotenv()

LLM_API_KEY = os.getenv("LLM_API_KEY")

# 初始化模型
chat = ChatTongyi(model="qwen3-max",api_key=LLM_API_KEY)

# 准备消息list
messages = [
    HumanMessage(content="给我写一首唐诗")
]

for chunk in chat.stream(input=messages):
    print(chunk.content, end="", flush=True)
```

## 演示SystemMessage + HumanMessage

```python
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.messages import HumanMessage,SystemMessage
from dotenv import load_dotenv
import os

load_dotenv()

LLM_API_KEY = os.getenv("LLM_API_KEY")

# 初始化模型
chat = ChatTongyi(model="qwen3-max",api_key=LLM_API_KEY)

# 准备消息list
messages = [
    SystemMessage(content="你是蔡徐坤"),
    HumanMessage(content="给我唱一首只因你太美")
]

# 流式输出
for chunk in chat.stream(input=messages):
    print(chunk.content, end="", flush=True)
```

## 演示SystemMessage + HumanMessage + AIMessage

```python
from langchain_community.chat_models.tongyi import ChatTongyi
from langchain_core.messages import HumanMessage,SystemMessage,AIMessage
from dotenv import load_dotenv
import os

load_dotenv()

LLM_API_KEY = os.getenv("LLM_API_KEY")

# 得到模型对象，qwen3-max就是聊天模型
chat = ChatTongyi(model="qwen3-max",api_key=LLM_API_KEY)

# 准备消息列表
messages = [
    SystemMessage(content="你是唐代诗人"),
    HumanMessage(content="给我来一首古诗"),
    AIMessage(content="床前明月光"),
    HumanMessage(content="接着说下一句")
]

# 调用stream流式执行
res = chat.stream(input=messages)

# for循环迭代打印输出，通过.content来获取内容
for chunk in res:
    print(chunk.content, end="", flush=True)
```

## 本地Ollma演示

```python
from langchain_ollama import ChatOllama
from langchain_core.messages import HumanMessage,SystemMessage,AIMessage

# 初始化模型
chat = ChatOllama(model="qwen2.5:0.5B")

# 准备消息list
messages = [
    SystemMessage(content="你是唐代诗人"),
    HumanMessage(content="给我来一首古诗"),
    AIMessage(content="床前明月光"),
    HumanMessage(content="接着说下一句")
]

# 流式输出
for chunk in chat.stream(input=messages):
    print(chunk.content, end="", flush=True)
```

## 消息的简写形式

SystemMessage、HumanMessage、AIMessage 可以有如下的简写形式

通过二元元组封装信息：

- 第一个元素为角色字符串：system/human/ai
- 第二个元素为内容

```python
from langchain_ollama import ChatOllama

# 初始化模型
chat = ChatOllama(model="qwen2.5:0.5B")

# 准备消息列表
messages = [
    # （角色，内容）角色：system/human/ai
    ("system","你是唐代诗人"),
    ("human","给我来一首古诗上半句"),
    ("ai","床前明月光"),
    ("human","接着只说下一句即可"),
]

# 调用stream流式执行
res = chat.stream(input=messages)

# 流式输出
for chunk in res:
    print(chunk.content, end="", flush=True)
```

区别和优势在于，使用类对象的方式，如下：

```python
messages = [
    SystemMessage(content="内容..."),
    HumanMessage(content="内容..."),
    AIMessage(content="内容..."),
]
```

是静态的，一步到位直接就得到了 Message 类的类对象

简写形式如下：

```python
messages = [
    ("system", "内容..."),
    ("human", "内容..."),
    ("ai", "内容..."),
]
```

是动态的，需要在运行时由 LangChain 内部机制转换为 Message 类对象

好处就在于，简写形式避免导包、写起来更简单，更重要的是支持：

```python
messages = [
    ("system", "今天的天气是{weather}"),
    ("human", "我的名字是：{name}"),
    ("ai", "欢迎{lastname}先生"),
]
```

由于是动态，需要转换步骤

所以简写形式支持内部填充`{变量}`占位

可在运行时填充具体值（后续学习提示词模板时用到）
