---
sidebar_position: 7
description: LangChain 模型的流式输出
---

# 流式输出

## 流式输出结果

如果需要流式输出结果，需要将模型的invoke方法改为stream方法即可。

- invoke方法：一次型返回完整结果
- stream方法：逐段返回结果，流式输出

```python
from langchain_community.llms.tongyi import Tongyi
from dotenv import load_dotenv
import os

load_dotenv()

LLM_API_KEY = os.getenv("LLM_API_KEY")

model = Tongyi(model="qwen-max", api_key=LLM_API_KEY)

# 调用stream方法获得流式输出
res = model.stream(input="你是谁呀能做什么？")

for chunk in res:
    print(chunk, end="", flush=True)

```

本地ollama流式输出：

```python
from langchain_ollama import OllamaLLM

llm = OllamaLLM(model="qwen2.5:0.5b")

res = llm.stream(input="你是谁你在干什么")

for chunk in res:
    print(chunk, end="", flush=True)

```

## 总结

模型对象有 2 个方法去调用模型：

- invoke，调用模型，一次性返回完整结果
- stream，调用模型，逐段流式输出

这两个方法是新版 LangChain（1.0 版本后）中基于 Runnable 接口的通用核心方法。

绝大多数组件（如提示词模板、链、向量检索、工具调用等，后续学习）都支持这两个方法，这也是 LangChain 设计的核心统一范式。
