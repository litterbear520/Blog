---
sidebar_position: 1
---

# Nano-Banana

è¿™é‡Œæ€»ç»“äº† Nano-Banana çš„ç»˜å›¾æŠ€å·§ï¼Œå¹¶è®°å½•è‡ªå·±åŠ¨æ‰‹å®è·µçš„è¿‡ç¨‹ï¼Œæ„Ÿå…´è¶£å¯ä»¥çœ‹åŸæ–‡

[å†ä¹Ÿä¸æ‹…å¿ƒè®ºæ–‡ï¼Nano-Banana Proè®ºæ–‡ç»˜å›¾æœ€å…¨æ•™ç¨‹å‘å¸ƒ](https://mp.weixin.qq.com/s/IG8cITKAESi-vomyQiobZg)

## è®ºæ–‡ç»˜å›¾

### æ­¥éª¤ä¸€ï¼šé€»è¾‘æ„å»º

ç›®æ ‡ï¼šåˆ©ç”¨é€»è¾‘æ¨ç†èƒ½åŠ›å¼ºçš„LLMï¼ˆå¦‚Gemini 3 Pro,GPT-5,Claude 4.5ï¼‰å°†è®ºæ–‡å†…å®¹è½¬åŒ–ä¸ºä¸€ä»½[VISUAL SCHEMA]ã€‚   

å¤åˆ¶ä¸‹æ–¹çš„Promptï¼Œå¹¶å°†è®ºæ–‡æ‘˜è¦æˆ–æ–¹æ³•ç« èŠ‚çš„å†…å®¹é™„åœ¨æœ€åã€‚è¿™ä¸€æ­¥çš„æ ¸å¿ƒåœ¨äºå°†æŠ½è±¡çš„ç®—æ³•é€»è¾‘è½¬åŒ–ä¸ºç»˜å›¾æ¨¡å‹èƒ½å¤Ÿç†è§£çš„"å¼ºç¡¬"ç‰©ç†æè¿°ã€‚

```markdown
# Role
ä½ æ˜¯ä¸€ä½ CVPR/NeurIPS é¡¶åˆŠçš„**è§†è§‰æ¶æ„å¸ˆ**ã€‚ä½ çš„æ ¸å¿ƒèƒ½åŠ›æ˜¯å°†æŠ½è±¡çš„è®ºæ–‡é€»è¾‘è½¬åŒ–ä¸º**å…·ä½“çš„ã€ç»“æ„åŒ–çš„ã€å‡ ä½•çº§çš„è§†è§‰æŒ‡ä»¤**ã€‚

# Objective
é˜…è¯»æˆ‘æä¾›çš„è®ºæ–‡å†…å®¹ï¼Œè¾“å‡ºä¸€ä»½ **[VISUAL SCHEMA]**ã€‚è¿™ä»½ Schema å°†è¢«ç›´æ¥å‘é€ç»™ AI ç»˜å›¾æ¨¡å‹ï¼Œå› æ­¤å¿…é¡»ä½¿ç”¨**å¼ºç¡¬çš„ç‰©ç†æè¿°**ã€‚

# Phase 1: Layout Strategy Selector (å…³é”®æ­¥éª¤ï¼šå¸ƒå±€å†³ç­–)
åœ¨ç”Ÿæˆ Schema ä¹‹å‰ï¼Œè¯·å…ˆåˆ†æè®ºæ–‡é€»è¾‘ï¼Œä»ä»¥ä¸‹**å¸ƒå±€åŸå‹**ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸€ä¸ªï¼ˆæˆ–ç»„åˆï¼‰ï¼š
1.  **Linear Pipeline**: å·¦â†’å³æµå‘ (é€‚åˆ Data Processing, Encoding-Decoding)ã€‚
2.  **Cyclic/Iterative**: ä¸­å¿ƒåŒ…å«å¾ªç¯ç®­å¤´ (é€‚åˆ Optimization, RL, Feedback Loops)ã€‚
3.  **Hierarchical Stack**: ä¸Šâ†’ä¸‹æˆ–ä¸‹â†’ä¸Šå †å  (é€‚åˆ Multiscale features, Tree structures)ã€‚
4.  **Parallel/Dual-Stream**: ä¸Šä¸‹å¹³è¡Œçš„åŒæµç»“æ„ (é€‚åˆ Multi-modal fusion, Contrastive Learning)ã€‚
5.  **Central Hub**: ä¸€ä¸ªæ ¸å¿ƒæ¨¡å—è¿æ¥å››å‘¨ç»„ä»¶ (é€‚åˆ Agent-Environment, Knowledge Graphs)ã€‚

# Phase 2: Schema Generation Rules
1.  **Dynamic Zoning**: æ ¹æ®é€‰æ‹©çš„å¸ƒå±€ï¼Œå®šä¹‰ 2-5 ä¸ªç‰©ç†åŒºåŸŸ (Zones)ã€‚ä¸è¦å±€é™äº 3 ä¸ªã€‚
2.  **Internal Visualization**: å¿…é¡»å®šä¹‰æ¯ä¸ªåŒºåŸŸå†…éƒ¨çš„â€œç‰©ä½“â€ (Icons, Grids, Trees)ï¼Œç¦æ­¢ä½¿ç”¨æŠ½è±¡æ¦‚å¿µã€‚
3.  **Explicit Connections**: å¦‚æœæ˜¯å¾ªç¯è¿‡ç¨‹ï¼Œå¿…é¡»æ˜ç¡®æè¿° "Curved arrow looping back from Zone X to Zone Y"ã€‚

# Output Format (The Golden Schema)
è¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹ Markdown ç»“æ„è¾“å‡ºï¼š

---BEGIN PROMPT---

[Style & Meta-Instructions]
High-fidelity scientific schematic, technical vector illustration, clean white background, distinct boundaries, academic textbook style. High resolution 4k, strictly 2D flat design with subtle isometric elements.

[LAYOUT CONFIGURATION]
* **Selected Layout**: [ä¾‹å¦‚ï¼šCyclic Iterative Process with 3 Nodes]
* **Composition Logic**: [ä¾‹å¦‚ï¼šA central triangular feedback loop surrounded by input/output panels]
* **Color Palette**: Professional Pastel (Azure Blue, Slate Grey, Coral Orange, Mint Green).

[ZONE 1: LOCATION - LABEL]
* **Container**: [å½¢çŠ¶æè¿°, e.g., Top-Left Panel]
* **Visual Structure**: [å…·ä½“æè¿°, e.g., A stack of documents]
* **Key Text Labels**: "[Text 1]"

[ZONE 2: LOCATION - LABEL]
* **Container**: [å½¢çŠ¶æè¿°, e.g., Central Circular Engine]
* **Visual Structure**: [å…·ä½“æè¿°, e.g., A clockwise loop connecting 3 internal modules: A (Gear), B (Graph), C (Filter)]
* **Key Text Labels**: "[Text 2]", "[Text 3]"

[ZONE 3: LOCATION - LABEL]
... (Add Zone 4/5 if necessary based on layout)

[CONNECTIONS]
1.  [æè¿°è¿æ¥çº¿, e.g., A curved dotted arrow looping from Zone 2 back to Zone 1 labeled "Feedback"]
2.  [æè¿°è¿æ¥çº¿, e.g., A wide flow arrow from Zone 2 to Zone 3]

---END PROMPT---

# Input Data
[åœ¨æ­¤å¤„ç²˜è´´ä½ çš„è®ºæ–‡å†…å®¹]
```

è¿™é‡Œæˆ‘å°†è¿™ç¯‡æœ‰åçš„æ–¯å¦ç¦AIå°é•‡çš„è®ºæ–‡[Generative Agents: Interactive Simulacra of Human Behavior](https://arxiv.org/abs/2304.03442)ä½œä¸ºè¾“å…¥ã€‚  
æˆ‘è¾“å…¥çš„æ˜¯ç¬¬å››ç« èŠ‚çš„å…¨éƒ¨å†…å®¹ï¼Œå› ä¸ºç¬¬å››ç« æ˜¯æ•´ç¯‡è®ºæ–‡çš„ç²¾åï¼Œå®ƒä»‹ç»äº†Agentçš„æ¶æ„ã€‚

<details>
<summary>ç‚¹å‡»å±•å¼€æŸ¥çœ‹è®ºæ–‡è¾“å…¥ç¤ºä¾‹</summary>

```markdown
# 4 GENERATIVE AGENT ARCHITECTURE

Generative agents aim to provide a framework for behavior in an open world: one that can engage in interactions with other agents and react to changes in the environment. Generative agents take their current environment and past experiences as input and generate behavior as output. Underlying this behavior is a novel agent architecture that combines a large language model with mechanisms for synthesizing and retrieving relevant information to condition the language modelâ€™s output. Without these mechanisms, large language models can output behavior, but the resulting agents may not react based on the agentâ€™s past experiences, may not make important inferences, and may not maintain long-term coherence. Challenges with long-term planning and coherence remain [19] even with todayâ€™s most performant models such as GPT-4. Because generative agents produce large streams of events and memories that must be retained, a core challenge of our architecture is to ensure that the most relevant pieces of the agentâ€™s memory are retrieved and synthesized when needed.

At the center of our architecture is the memory stream, a database that maintains a comprehensive record of an agentâ€™s experience. From the memory stream, records are retrieved as relevant to plan the agentâ€™s actions and react appropriately to the environment. Records are recursively synthesized into higher- and higher-level reflections that guide behavior. Everything in the architecture is recorded and reasoned over as a natural language description, allowing the architecture to leverage a large language model.

Our current implementation utilizes the gpt3.5-turbo version of ChatGPT [77]. We expect that the architectural basics of generative agentsâ€”memory, planning, and reflectionâ€”will likely remain the same as language models improve. Newer language models (e.g., GPT-4) will continue to expand the expressive power and performance of the prompts that underpin generative agents. As of writing, however, GPT-4â€™s API was invitation-only, so our agents use ChatGPT.

## 4.1 Memory and Retrieval

*Challenge:* Creating generative agents that can simulate human behavior requires reasoning about a set of experiences that is far larger than what should be described in a prompt, as the full memory stream can distract the model and does not even currently fit into the limited context window. Consider the Isabella agent answering the question, â€œWhat are you passionate about these days?â€ Summarizing all of Isabellaâ€™s experiences to fit in the limited context window of the language model produces an uninformative response, where Isabella discusses topics such as collaborations for events and projects and cleanliness and organization in a cafe. Instead of summarizing, the memory stream described below surfaces relevant memories, resulting in a more informative and specific response that mentions Isabellaâ€™s passion for making people feel welcome and included, planning events and creating an atmosphere that people can enjoy, such as the Valentineâ€™s Day party.

*Approach:* The *memory stream* maintains a comprehensive record of the agentâ€™s experience. It is a list of memory objects, where each object contains a natural language description, a creation timestamp, and a most recent access timestamp. The most basic element of the memory stream is an *observation*, which is an event directly perceived by an agent. Common observations include behaviors performed by the agent themselves or behaviors that agents perceive being performed by other agents or non-agent objects. For instance, Isabella Rodriguez, who works at a coffee shop, might accrue the following observations over time: (1) Isabella Rodriguez is setting out the pastries, (2) Maria Lopez is studying for a Chemistry test while drinking coffee, (3) Isabella Rodriguez and Maria Lopez are conversing about planning a Valentineâ€™s day party at Hobbs Cafe, (4) The refrigerator is empty.

Our architecture implements a retrieval function that takes the agentâ€™s current situation as input and returns a subset of the memory stream to pass on to the language model. There are many possible implementations of a retrieval function, depending on what is important for the agent to consider when deciding how to act. In our context, we focus on three main components that, together, produce effective results.

*Recency* assigns a higher score to memory objects that were recently accessed, so that events from a moment ago or this morning are likely to remain in the agentâ€™s attentional sphere. In our implementation, we treat recency as an exponential decay function over the number of sandbox game hours since the memory was last retrieved. Our decay factor is 0.995.

*Importance* distinguishes mundane from core memories by assigning a higher score to memory objects that the agent believes to be important. For instance, a mundane event, such as eating breakfast in oneâ€™s room, would yield a low importance score, whereas a breakup with oneâ€™s significant other would yield a high score. There are many possible implementations of an importance score; we find that directly asking the language model to output an integer score is effective. The full prompt appears below:

> On the scale of 1 to 10, where 1 is purely mundane
> (e.g., brushing teeth, making bed) and 10 is
> extremely poignant (e.g., a break up, college
> acceptance), rate the likely poignancy of the
> following piece of memory.
> Memory: buying groceries at The Willows Market
> and Pharmacy
> Rating: <fill in>

This prompt returns an integer value of 2 for â€œcleaning up the roomâ€ and 8 for â€œasking your crush out on a date.â€ The importance score is generated at the time the memory object is created.

*Relevance* assigns a higher score to memory objects that are related to the current situation. What is relevant depends on the answer to, â€œRelevant to what?â€, so we condition relevance on a *query* memory. If the query, for example, is that a student is discussing what to study for a chemistry test with a classmate, memory objects about their breakfast should have low relevance, whereas memory objects about the teacher and schoolwork should have high relevance. In our implementation, we use the language model to generate an embedding vector of the text description of each memory. Then, we calculate relevance as the cosine similarity between the memoryâ€™s embedding vector and the query memoryâ€™s embedding vector.

To calculate the final retrieval score, we normalize the recency, relevance, and importance scores to the range of [0, 1] using min-max scaling. The retrieval function scores all memories as a weighted combination of the three elements: 

$$score = \alpha_{recency} \cdot recency + \alpha_{importance} \cdot importance + \alpha_{relevance} \cdot relevance$$

In our implementation, all $\alpha$s are set to 1. The top-ranked memories that fit within the language modelâ€™s context window are included in the prompt.

## 4.2 Reflection

*Challenge:* Generative agents, when equipped with only raw observational memory, struggle to generalize or make inferences. Consider a scenario in which Klaus Mueller is asked by the user: â€œIf you had to choose one person of those you know to spend an hour with, who would it be?" With access to only observational memory, the agent simply chooses the person with whom Klaus has had the most frequent interactions: Wolfgang, his college dorm neighbor. Unfortunately, Wolfgang and Klaus only ever see each other in passing, and do not have deep interactions. A more desirable response requires that the agent generalize from memories of Klaus spending hours on a research project to generate a higher-level reflection that Klaus is passionate about research, and likewise recognize Maria putting in effort into her own research (albeit in a different field), enabling a reflection that they share a common interest. With the approach below, when Klaus is asked who to spend time with, Klaus chooses Maria instead of Wolfgang.

*Approach:* We introduce a second type of memory, which we call a *reflection*. Reflections are higher-level, more abstract thoughts generated by the agent. Because they are a type of memory, they are included alongside other observations when retrieval occurs. Reflections are generated periodically; in our implementation, we generate reflections when the sum of the importance scores for the latest events perceived by the agents exceeds a threshold (150 in our implementation). In practice, our agents reflected roughly two or three times a day.

The first step in reflection is for the agent to determine *what* to reflect on, by identifying questions that can be asked given the agentâ€™s recent experiences. We query the large language model with the 100 most recent records in the agentâ€™s memory stream (e.g., â€œKlaus Mueller is reading a book on gentrificationâ€, â€œKlaus Mueller is conversing with a librarian about his research projectâ€, â€œdesk at the library is currently unoccupiedâ€) and prompt the language model, â€œGiven only the information above, what are 3 most salient high-level questions we can answer about the subjects in the statements?â€ The modelâ€™s response generates candidate questions: for example, *What topic is Klaus Mueller passionate about?* and *What is the relationship between Klaus Mueller and Maria Lopez?* We use these generated questions as queries for retrieval, and gather relevant memories (including other reflections) for each question. Then we prompt the language model to extract insights and cite the particular records that served as evidence for the insights. The full prompt is as follows:

> Statements about Klaus Mueller
> 1. Klaus Mueller is writing a research paper
> 2. Klaus Mueller enjoys reading a book
> on gentrification
> 3. Klaus Mueller is conversing with Ayesha Khan
> about exercising [...]
> What 5 high-level insights can you infer from
> the above statements? (example format: insight
> (because of 1, 5, 3))

This process generates statements such as *Klaus Mueller is dedicated to his research on gentrification (because of 1, 2, 8, 15)*. We parse and store the statement as a reflection in the memory stream, including pointers to the memory objects that were cited.

Reflection explicitly allows the agents to reflect not only on their observations but also on other reflections: for example, the second statement about Klaus Mueller above is a reflection that Klaus previously had, not an observation from his environment. As a result, agents generate trees of reflections: the leaf nodes of the tree represent the base observations, and the non-leaf nodes represent thoughts that become more abstract and higher-level the higher up the tree they are.

## 4.3 Planning and Reacting

*Challenge:* While a large language model can generate plausible behavior in response to situational information (e.g., [46, 80]), agents need to plan over a longer time horizon to ensure that their sequence of actions is coherent and believable. If we prompt a language model with Klausâ€™s background, describe the time, and ask what action he ought to take at the given moment, Klaus would eat lunch at 12 pm, but then again at 12:30 pm and 1 pm, despite having already eaten his lunch twice. Optimizing for believability in the moment sacrifices believability over time. To overcome this issue, *planning* is essential. With the approach described below, Klausâ€™s afternoon plan is less gluttonous: he has lunch at Hobbs Cafe while reading at 12pm, works on his research paper at the school library at 1pm, and takes a break for a walk in the park at 3pm.

*Approach:* Plans describe a future sequence of actions for the agent, and help keep the agentâ€™s behavior consistent over time. A plan includes a location, a starting time, and a duration. For instance, Klaus Mueller, who is dedicated in his research and has an impending deadline, may choose to spend his day working at his desk drafting his research paper. An entry in a plan might state, for example: `for 180 minutes from 9am, February 12th, 2023, at Oak Hill College Dorm: Klaus Muellerâ€™s room: desk, read and take notes for research paper`. Like reflections, plans are stored in the memory stream and are included in the retrieval process. This allows the agent to consider observations, reflections, and plans all together when deciding how to behave. Agents may change their plans midstream if needed.

It would be unrealistic and uninteresting for an artist agent to plan on painting while sitting at a pharmacy counter for four hours without moving. A more desirable plan would involve the agent taking the necessary time to gather materials, mix paint, take breaks, and clean up during the four-hour period in their home studio. To create such plans, our approach starts top-down and then recursively generates more detail. The first step is to create a plan that outlines the dayâ€™s agenda in broad strokes. To create the initial plan, we prompt the language model with the agentâ€™s summary description (e.g., name, traits, and a summary of their recent experiences) and a summary of their previous day. A full example prompt is below, which is unfinished at the bottom for the language model to complete:

> Name: Eddy Lin (age: 19)
> Innate traits: friendly, outgoing, hospitable
> Eddy Lin is a student at Oak Hill College studying
> music theory and composition. He loves to explore
> different musical styles and is always looking for
> ways to expand his knowledge. Eddy Lin is working
> on a composition project for his college class. He
> is taking classes to learn more about music theory.
> Eddy Lin is excited about the new composition he
> is working on but he wants to dedicate more hours
> in the day to work on it in the coming days
> On Tuesday February 12, Eddy 1) woke up and
> completed the morning routine at 7:00 am, [. . . ]
> 6) got ready to sleep around 10 pm.
> Today is Wednesday February 13. Here is Eddyâ€™s
> plan today in broad strokes: 1)

This generates a rough sketch of the agentâ€™s plan for a day, divided into five to eight chunks: â€œ1) wake up and complete the morning routine at 8:00 am, 2) go to Oak Hill College to take classes starting 10:00 am, [. . . ] 5) work on his new music composition from 1:00 pm to 5:00 pm, 6) have dinner at 5:30 pm, 7) finish school assignments and go to bed by 11:00 pm.â€

The agent saves this plan in the memory stream and then recursively decomposes it to create finer-grained actions, first into hour-long chunks of actionsâ€”Eddyâ€™s plan to `work on his new music composition from 1:00 pm to 5:00 pm` becomes `1:00 pm: start by brainstorming some ideas for his music composition [...] 4:00 pm: take a quick break and recharge his creative energy before reviewing and polishing his composition`. We then recursively decompose this again into 5â€“15 minute chunks: e.g., `4:00 pm: grab a light snack, such as a piece of fruit, a granola bar, or some nuts. 4:05 pm: take a short walk around his workspace [...] 4:50 pm: take a few minutes to clean up his workspace`. This process can be adjusted to match the desired granularity.

### 4.3.1 Reacting and Updating Plans.

Generative agents operate in an action loop where, at each time step, they perceive the world around them and those perceived observations are stored in their memory stream. We prompt the language model with these observations to decide whether the agent should continue with their existing plan, or react. Standing at an easel and painting, for example, might trigger an observation of the easel, but this is unlikely to prompt a reaction. However, if Eddyâ€™s father John records that he sees Eddy taking a short walk in the house garden, the outcome is different. The prompt is below, with `[Agentâ€™s Summary Description]` standing in for a dynamically-generated, paragraph-long summary of the agentâ€™s overall goals and disposition, which is described in Appendix A:

> [Agentâ€™s Summary Description]
> It is February 13, 2023, 4:56 pm.
> John Linâ€™s status: John is back home early from
> work.
> Observation: John saw Eddy taking a short walk
> around his workplace.
> Summary of relevant context from Johnâ€™s memory:
> Eddy Lin is Johnâ€™s Linâ€™s son. Eddy Lin has been
> working on a music composition for his class. Eddy
> Lin likes to walk around the garden when he is
> thinking about or listening to music.
> Should John react to the observation, and if so,
> what would be an appropriate reaction?

The context summary is generated through two prompts that retrieve memories via the queries â€œWhat is [observer]â€™s relationship with the [observed entity]?â€ and â€œ[Observed entity] is [action status of the observed entity]â€, and their answers summarized together. The output suggests that `John could consider asking Eddy about his music composition project`. We then regenerate the agentâ€™s existing plan starting from the time when the reaction takes place. Finally, if the action indicates an interaction between agents, we generate their dialogue.

### 4.3.2 Dialogue.

Agents converse as they interact with each other. We generate agentsâ€™ dialogue by conditioning their utterances on their memories about each other. For example, when John initiates his conversation with Eddy, we generate Johnâ€™s first utterance by using his summarized memory about Eddy and the intended reaction when he decided to ask Eddy about his composition project:

> [Agentâ€™s Summary Description]
> It is February 13, 2023, 4:56 pm.
> John Linâ€™s status: John is back home early from
> work.
> Observation: John saw Eddy taking a short walk
> around his workplace.
> Summary of relevant context from Johnâ€™s memory:
> Eddy Lin is Johnâ€™s Linâ€™s son. Eddy Lin has been
> working on a music composition for his class. Eddy
> Lin likes to walk around the garden when he is
> thinking about or listening to music.
> John is asking Eddy about his music composition
> project. What would he say to Eddy?

The result: â€œHey Eddy, howâ€™s the music composition project for your class coming along?â€ From Eddyâ€™s perspective, John initiating the dialogue is seen as an event to which he may want to react. So, just as John did, Eddy retrieves and summarizes his memory about his relationship with John, as well as his memory that may be related to Johnâ€™s last utterance in the dialogue. If he decides to respond, we generate Eddyâ€™s utterance using his summarized memory and the current dialogue history:

> [Agentâ€™s Summary Description]
> It is February 13, 2023, 4:56 pm.
> Eddy Linâ€™s status: Eddy is taking a short walk
> around his workplace.
> Observation: John is initiating a conversation
> with Eddy.
> Summary of relevant context from Eddyâ€™s memory:
> John Lin is Eddy Linâ€™s father. John Lin is caring
> and is interested to learn more about Eddy Linâ€™s
> school work. John Lin knows that Eddy Lin is
> working on a music composition.
> Here is the dialogue history:
> John: Hey Eddy, howâ€™s the music composition project
> for your class coming along?
> How would Eddy respond to John?

This generates Eddyâ€™s response: â€œHey Dad, itâ€™s going well. Iâ€™ve been taking walks around the garden to clear my head and get some inspiration.â€ The continuation of this dialogue is generated using the same mechanism until one of the two agents decides to end the dialogue.
```

</details>

**å…¬å¼é¢„è§ˆï¼š**

$$score = \alpha_{recency} \cdot recency + \alpha_{importance} \cdot importance + \alpha_{relevance} \cdot relevance$$ 

æˆ‘äº¤ç»™äº†Gemini 3 proï¼Œæ‹¿åˆ°äº†ä»¥ä¸‹è¿”å›ç»“æœï¼š

<details>
<summary>ç‚¹å‡»å±•å¼€æŸ¥çœ‹Gemini 3 Proç”Ÿæˆçš„Prompt</summary>

```markdown
Based on the detailed architectural logic of the "Generative Agents" paper (Section 4), here is the **VISUAL SCHEMA** designed for a high-end technical illustration.

This schema translates the abstract concepts of Memory Streams, Retrieval Scoring, and Hierarchical Planning into tangible, geometric structures.

---BEGIN PROMPT---

[Style & Meta-Instructions]
High-fidelity scientific schematic, technical vector illustration, clean white background, distinct boundaries, academic textbook style. High resolution 4k, strictly 2D flat design with subtle isometric elements. Deep technical aesthetic, similar to neural network architecture diagrams but with specific semantic modules.

[LAYOUT CONFIGURATION]
* **Selected Layout**: Central Hub with Cyclic Feedback
* **Composition Logic**: A massive central "Memory Stream" pillar acts as the backbone. Input flows in from the left, travels up through a "Retrieval Filter," branches right into "Cognition/Planning," and loops back to the center as new actions.
* **Color Palette**: Professional Pastel (Mint Green for Observations, Azure Blue for Reflections, Coral Orange for Plans, Slate Grey for Structure).

[ZONE 1: CENTER - THE MEMORY STREAM]
* **Container**: A tall, vertical, semi-transparent cylinder or data-scroll standing in the center.
* **Visual Structure**: An infinite stack of floating horizontal cards (distinct blocks). The cards at the bottom are densely packed (older memories), while cards at the top are bright and spaced out (recent memories).
* **Key Text Labels**: "Memory Object", "Creation Timestamp", "Access Timestamp".
* **Detail**: Some cards have small icons: 'Eye' (Observation), 'Bulb' (Reflection), 'Grid' (Plan).

[ZONE 2: TOP - THE RETRIEVAL ENGINE]
* **Container**: A triangular prism or mechanical filter unit hovering directly above the Memory Stream.
* **Visual Structure**: Three distinct gauge/lens components focusing a beam of light onto the Memory Stream cards.
    1.  **Lens A (Recency)**: Symbolized by a decay curve graph or clock icon.
    2.  **Lens B (Importance)**: Symbolized by a weighted scale or exclamation mark.
    3.  **Lens C (Relevance)**: Symbolized by two magnets or a vector cosine angle icon.
* **Key Text Labels**: "Score = Î±(Recency) + Î±(Importance) + Î±(Relevance)".

[ZONE 3: RIGHT - COGNITION & PLANNING]
* **Container**: A large split rectangular panel connected to the Retrieval Engine.
* **Visual Structure (Upper Half - Reflection)**: A tree structure growing upwards. Leaf nodes (Observations) connect to branch nodes (Synthesized Thoughts). It looks like a dendrogram.
* **Visual Structure (Lower Half - Planning)**: A hierarchical GANTT chart. A top bar shows a broad task (e.g., "Write Paper"), decomposing into smaller bars below it (e.g., "Read Notes", "Draft Section").
* **Key Text Labels**: "Reflection Tree", "Recursive Decomposition", "Coarse-to-Fine Plan".

[ZONE 4: LEFT - PERCEPTION & ENVIRONMENT]
* **Container**: A floating isometric cube on the left side.
* **Visual Structure**: A miniature "Sandbox" scene showing a pixel-art style Avatar (John or Isabella) standing near a coffee machine. A generic "Observer Camera" icon floats above them.
* **Key Text Labels**: "Sandbox Environment", "Raw Observation", "Current State".

[CONNECTIONS]
1.  **Input Flow**: A solid line arrow from Zone 4 (Environment) entering the bottom of Zone 1 (Memory Stream) labeled "New Observation".
2.  **Retrieval Flow**: A beam of light from Zone 2 (Retrieval Engine) highlighting specific cards in Zone 1, then pulling them into Zone 3.
3.  **Action Loop**: A curved arrow exiting Zone 3 (Planning), labeled "Act / React", looping back to modify Zone 4 (Environment).
4.  **Feedback Loop**: A dotted arrow from Zone 3 (Reflection) inserting new "Thought Cards" back into the top of Zone 1 (Memory Stream).

---END PROMPT---
```

</details>

### æ­¥éª¤äºŒï¼šç»˜å›¾æ¸²æŸ“
ç›®æ ‡ï¼šåˆ©ç”¨Nano-Banana Proçš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›ï¼Œå°†è“å›¾è½¬åŒ–ä¸ºåƒç´ ã€‚

å¤åˆ¶ä¸‹é¢çš„é€šç”¨æ¨¡æ¿ã€‚æ³¨æ„ï¼Œè¿™é‡Œéœ€è¦å°†æ­¥éª¤ä¸€ç”Ÿæˆçš„---BEGIN PROMPT---åˆ°---END PROMPT---ä¹‹é—´çš„å†…å®¹ï¼ˆåŒ…å«æ–¹æ‹¬å·å†…çš„è‹±æ–‡ï¼‰å®Œæ•´ç²˜è´´è¿›å»å³å¯ï¼Œæ— éœ€åšä»»ä½•ä¿®æ”¹ã€‚

```markdown
**Style Reference & Execution Instructions:**

1.  **Art Style (Visio/Illustrator Aesthetic):**
    Generate a **professional academic architecture diagram** suitable for a top-tier computer science paper (CVPR/NeurIPS).
    * **Visuals:** Flat vector graphics, distinct geometric shapes, clean thin outlines, and soft pastel fills (Azure Blue, Slate Grey, Coral Orange).
    * **Layout:** Strictly follow the spatial arrangement defined below.
    * **Vibe:** Technical, precise, clean white background. NOT hand-drawn, NOT photorealistic, NOT 3D render, NO shadows/shading.

2.  **CRITICAL TEXT CONSTRAINTS (Read Carefully):**
    * **DO NOT render meta-labels:** Do not write words like "ZONE 1", "LAYOUT CONFIGURATION", "Input", "Output", or "Container" inside the image. These are structural instructions for YOU, not text for the image.
    * **ONLY render "Key Text Labels":** Only text inside double quotes (e.g., "[Text]") listed under "Key Text Labels" should appear in the diagram.
    * **Font:** Use a clean, bold Sans-Serif font (like Roboto or Helvetica) for all labels.

3.  **Visual Schema Execution:**
    Translate the following structural blueprint into the final image:

[åœ¨æ­¤å¤„ç›´æ¥ç²˜è´´ Step 1 ç”Ÿæˆçš„ ---BEGIN PROMPT--- ... ---END PROMPT--- å†…å®¹ï¼ˆåŒ…å«æ–¹æ‹¬å·å†…çš„è‹±æ–‡ï¼‰]
```

å°†æ­¥éª¤ä¸€è¾“å‡ºçš„æç¤ºè¯ä¸¢ç»™Nano-Banana Proï¼Œä»¥ä¸‹æ˜¯æœ€ç»ˆç”Ÿæˆæ•ˆæœï¼š

![Nano-Banana Generated Image](./image/nano-banana-result.png)

### æ­¥éª¤ä¸‰ï¼šäº¤äº’å¼å¾®è°ƒä¸è¿­ä»£

å½“æ‹¿åˆ°æ­¥éª¤äºŒç”Ÿæˆçš„åˆç¨¿åï¼Œå‘ç°å¹¶ä¸æ»¡æ„åï¼Œå¯ä»¥æ ¹æ®å®é™…æƒ…å†µé€‰æ‹©ä¸åŒçš„æ“ä½œè·¯å¾„ï¼š


:::info ğŸ’¡å…³äºæŠ½å¡çš„æœ‰æ•ˆæ€§

é€šè¿‡æµ‹è¯•å‘ç°ï¼ŒæŠ½å¡å¯¹æ•´ä½“çš„å¸ƒå±€å’Œé£æ ¼æ”¹åŠ¨ä¸ä¼šç‰¹åˆ«å¤§ï¼Œä¸è¿‡å¯èƒ½ä¼šå¯¹æŸäº›çº¿æ¡çš„è·¯å¾„ã€æŸäº›å…ƒç´ çš„é¢œè‰²ã€å›¾å½¢çš„ç»†èŠ‚æœ‰äº›æ”¹å˜ã€‚

ä½ å¯ä»¥æŠ½å¡é€‰æ‹©ä½ æœ€å–œæ¬¢çš„ä¸€å¼ ï¼›æˆ–è€…å¦‚æœä½ æœ¬èº«æœ‰æ˜ç¡®çš„é…è‰²æ–¹æ¡ˆçš„è¯ï¼Œå¯ä»¥ç›´æ¥ç”¨è‡ªç„¶è¯­è¨€å»å¯¹è¿™å¼ åŸå›¾è¿›è¡Œä¿®æ”¹ã€‚

å¯¹æ¯”ä¸‹é¢ç›¸åŒæç¤ºè¯ç¬¬äºŒæ¬¡ç”Ÿæˆçš„ç»“æœï¼Œå…¶å®å·®åˆ«å¹¶ä¸æ˜¯ç‰¹åˆ«æ˜¾è‘—ã€‚å› æ­¤æ›´å¤šæƒ…å†µä¸‹ï¼Œå¯¹äºå¤§å¹…åº¦çš„è°ƒæ•´å¯ä»¥å»ä¼˜åŒ–æ­¥éª¤ä¸€çš„æç¤ºè¯ï¼›å°å¹…åº¦è°ƒæ•´ç›´æ¥è‡ªç„¶è¯­è¨€å»å‘½ä»¤ä¿®æ”¹å³å¯ã€‚
:::

![alt text](./image/image.png)

#### æƒ…å†µ Aï¼šæ•´ä½“å¸ƒå±€æ»¡æ„ï¼Œä½†ç»†èŠ‚æˆ–é£æ ¼æœ‰ç‘•ç–µ

æ­¤æ—¶åº”é‡‡å–â€œè‡ªç„¶è¯­è¨€ç¼–è¾‘â€ç­–ç•¥ã€‚ä½ å¯ä»¥ç›´æ¥åœ¨å¯¹è¯æ¡†ä¸­è¾“å…¥ä¿®æ”¹æŒ‡ä»¤ï¼Œæˆ–è€…åˆ©ç”¨ç•Œé¢ä¸Šçš„â€œé€‰ä¸­åŒºåŸŸç¼–è¾‘â€åŠŸèƒ½ã€‚æ¨¡å‹ä¼šåœ¨ä¿æŒç”»é¢ä¸»ä½“ç»“æ„ä¸å˜çš„å‰æä¸‹ï¼Œç²¾å‡†ä¿®æ”¹ä½ æŒ‡å®šçš„å…ƒç´ ã€‚ä¾‹å¦‚ï¼š

-   **ä¿®æ”¹å›¾æ ‡**ï¼šä½ å¯ä»¥è¯´ "Change the 'Gear' icon in the center to a 'Neural Network' icon"ï¼ˆæŠŠä¸­é—´çš„é½¿è½®æ¢æˆç¥ç»ç½‘ç»œå›¾æ ‡ï¼‰ï¼Œæˆ–è€… "Replace the robot head with a simple document symbol"ï¼ˆæŠŠæœºå™¨äººå¤´æ¢æˆæ–‡æ¡£ç¬¦å·ï¼‰ã€‚
-   **è°ƒæ•´é¢œè‰²**ï¼šä¾‹å¦‚ "Make the background of the left panel pure white instead of light blue"ï¼ˆæŠŠå·¦è¾¹é¢æ¿çš„èƒŒæ™¯æ”¹æˆçº¯ç™½ï¼‰ï¼Œæˆ– "Change the orange arrows to dark grey"ï¼ˆæŠŠæ©™è‰²ç®­å¤´æ”¹æˆæ·±ç°è‰²ï¼‰ã€‚
-   **é£æ ¼ç»Ÿä¸€**ï¼šå¦‚æœçº¿æ¡å¤ªç²—ï¼Œå¯ä»¥è¯´ "Make all lines thinner and cleaner"ï¼›å¦‚æœé˜´å½±å¹²æ‰°äº†è§†è§‰ï¼Œå¯ä»¥è¯´ "Remove the shading effect, make it completely flat 2D"ã€‚
-   **æ–‡å­—ä¿®æ­£**ï¼šå¦‚æœå‡ºç°æ‹¼å†™é”™è¯¯ï¼Œå¯ä»¥è¯´ "Correct the text 'ZONNE' to 'ZONE'"ã€‚å½“ç„¶ï¼Œå¦‚æœæ–‡å­—é”™è¯¯å¤ªä¸¥é‡ï¼Œæœ€ç¨³å¦¥çš„åŠæ³•æ˜¯ç›´æ¥è®© AI å»æ‰æ–‡å­— ("Remove the text labels")ï¼ŒåæœŸè‡ªå·±åœ¨ PPT ä¸­æ·»åŠ ã€‚

#### æƒ…å†µ Bï¼šæ•´ä½“å¸ƒå±€é”™è¯¯ (Layout Failure)

å¦‚æœä½ å‘ç°æœ¬è¯¥æ˜¯å¾ªç¯ç»“æ„çš„ç”»æˆäº†ç›´çº¿ï¼Œæˆ–è€…æ ¸å¿ƒé€»è¾‘å…³ç³»å®Œå…¨æåäº†ï¼Œè¿™æ—¶å€™ä¸è¦è¯•å›¾é€šè¿‡ä¿®è¡¥æ¥æŒ½æ•‘ã€‚è¿™é€šå¸¸æ„å‘³ç€æ­¥éª¤ä¸€ç”Ÿæˆçš„ [VISUAL SCHEMA] æœ¬èº«æè¿°ä¸å¤Ÿæ¸…æ™°ã€‚

æ­£ç¡®çš„åšæ³•æ˜¯å›åˆ°æ­¥éª¤ä¸€ã€‚æ£€æŸ¥å¹¶ä¿®æ”¹ Step 1 çš„ Promptï¼Œç¡®è®¤æ˜¯å¦é€‰é”™äº†`[LAYOUT CONFIGURATION]`ï¼Œæˆ–è€… Internal Visualization çš„æè¿°ä¸å¤Ÿå…·ä½“ã€‚ä½ å¯ä»¥ç›´æ¥å’ŒLLMå¯¹è¯ï¼Œè¦æ±‚å®ƒæŒ‰ç…§ä½ çš„è¦æ±‚ä¿®æ”¹`[VISUAL SCHEMA]`é‡æ–°ç”Ÿæˆè“å›¾åï¼Œå†æ¬¡è¿è¡Œæ­¥éª¤äºŒï¼Œå¾€å¾€èƒ½è§£å†³æ ¹æœ¬é—®é¢˜ã€‚

